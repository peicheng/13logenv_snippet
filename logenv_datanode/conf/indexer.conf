input {
	redis {
		host => "127.0.0.1"
			type => "redis-input"
# these settings should match the output of the agent
			data_type => "list"
			key => "logstash"
# We use json_event here since the sender is a logstash agent
			message_format => "json_event"
	}
}
filter {
#
#pattern format : Date LogLevel LoggerName LogMessage
#Date ISO
#
#2012-10-21 21:55:15,395 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.allocateBlock: /hbase/AEC_VRTL_UCHART/7be436e1c03be383a1382fcc910dae55/.tmp/.regioninfo. blk_3842205389437253622_27133386
#2012-10-21 21:55:15,401 INFO org.apache.hadoop.hdfs.StateChange: Removing lease on  file /hbase/AEC_VRTL_UCHART/7be436e1c03be383a1382fcc910dae55/.tmp/.regioninfo from client DFSClient_-771424537


	multiline {
		type => "namenode"
			pattern => "^20"
			negate => true
			what => "previous"
	}


	grok{

		type => "namenode"
			pattern => "^%{DATESTAMP:log_time}\s%{LOGLEVEL:log_evel}\s%{JAVACLASS:loggerName}:"
	}
	date{
		type => "namenode"
		log_time => "yyyy-MM-dd HH:mm:ss,SSS"
	}

	#
	multiline {
		type => "datanode"
			pattern => "^20"
			negate => true
			what => "previous"
	}


	grok{

		type => "datanode"
			pattern => "^%{DATESTAMP:log_time}\s%{LOGLEVEL:log_evel}\s%{JAVACLASS:loggerName}:"
	}
	date{
		type => "datanode"
		log_time => "yyyy-MM-dd HH:mm:ss,SSS"
	}

}

output {
	stdout { debug => true debug_format => "json"}

	elasticsearch {
		host => "127.0.0.1"
	}
}
